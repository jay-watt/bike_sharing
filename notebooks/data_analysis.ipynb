{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "\n",
    "from data_processing import load_train_and_test_data\n",
    "\n",
    "train_data, test_data = load_train_and_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Set Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_types = train_data.dtypes\n",
    "pd.DataFrame(data_types, columns=['Type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = train_data.isnull().sum()\n",
    "pd.DataFrame(missing_values, columns=[\"Count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = test_data.isnull().sum()\n",
    "pd.DataFrame(missing_values, columns=[\"Count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Duplicate Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_count = train_data.duplicated().sum()\n",
    "duplicates_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_count = test_data.duplicated().sum()\n",
    "duplicates_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Datetime Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import transform_datetime\n",
    "\n",
    "train_data_transformed = transform_datetime(train_data)\n",
    "train_data_transformed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = train_data_transformed.nunique()\n",
    "pd.DataFrame(unique_values, columns=[\"Unique Values Count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import perform_categorical_conversion\n",
    "\n",
    "train_data_processed = perform_categorical_conversion(train_data_transformed)\n",
    "data_types_after_processing = train_data_processed.dtypes\n",
    "pd.DataFrame(data_types_after_processing, columns=[\"Type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import TARGET_VARIABLES\n",
    "\n",
    "numerical = train_data_processed.select_dtypes(\n",
    "    include=['float64', 'int64', 'int32']).drop(columns=TARGET_VARIABLES).columns\n",
    "\n",
    "def calculate_numerical_statistics(columns, data):\n",
    "    basic_stats = data[columns].describe()\n",
    "    extended_stats = basic_stats.T\n",
    "    \n",
    "    extended_stats['median'] = data[columns].median()\n",
    "    extended_stats['variance'] = data[columns].var()\n",
    "    extended_stats['range'] = data[columns].max() - \\\n",
    "        data[columns].min()\n",
    "    extended_stats['iqr'] = data[columns].quantile(\n",
    "        0.75) - data[columns].quantile(0.25)\n",
    "    extended_stats['skewness'] = data[columns].skew()\n",
    "    extended_stats['kurtosis'] = data[columns].kurtosis()\n",
    "    \n",
    "    return extended_stats\n",
    "\n",
    "calculate_numerical_statistics(numerical, train_data_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def calculate_subplots_layout(columns):\n",
    "    num_features = len(columns)\n",
    "    num_cols = 2 if num_features < 5 else 3\n",
    "    num_rows = (num_features + num_cols - 1) // num_cols\n",
    "    \n",
    "    return num_rows, num_cols\n",
    "\n",
    "def rotate_xticklabels_if_long(ax, label_length_threshold=5, rotation_angle=45):\n",
    "    labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "    if any(len(label) > label_length_threshold for label in labels):\n",
    "        ax.tick_params(axis='x', labelrotation=rotation_angle)\n",
    "\n",
    "def plot_numerical_distributions(columns, data):\n",
    "    num_rows, num_cols = calculate_subplots_layout(columns)\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(8, 3*num_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, feature in enumerate(columns):\n",
    "        sns.histplot(data[feature], ax=axes[i], kde=True, edgecolor=None)\n",
    "        axes[i].set_title(feature)\n",
    "        axes[i].set_xlabel('')\n",
    "        rotate_xticklabels_if_long(axes[i])\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_numerical_distributions(numerical, train_data_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = train_data_processed.select_dtypes(\n",
    "    include=['category']).columns\n",
    "\n",
    "train_data_processed[categorical].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_categorical_distributions(categorical, data):\n",
    "    num_rows, num_cols = calculate_subplots_layout(categorical)\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(8, 6))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, col in enumerate(categorical):\n",
    "        sns.countplot(data=train_data_processed, x=col,\n",
    "                    ax=axes[i], order=train_data_processed[col].value_counts().index)\n",
    "        axes[i].set_title(col)\n",
    "        axes[i].set_xlabel('')\n",
    "        axes[i].set_ylabel('Count')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_categorical_distributions(categorical, train_data_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_numerical_statistics(TARGET_VARIABLES, train_data_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import transform_target_variable_data\n",
    "\n",
    "train_data_transformed = transform_target_variable_data(train_data_processed)\n",
    "calculate_numerical_statistics(TARGET_VARIABLES, train_data_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numerical_distributions(TARGET_VARIABLES, train_data_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical - Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_pearson_correlation(numerical_features1, numerical_features2, data):\n",
    "    results = []\n",
    "\n",
    "    for feature1 in numerical_features1:\n",
    "        for feature2 in numerical_features2:\n",
    "            if feature1 == feature2:\n",
    "                continue\n",
    "            corr = data[[feature1, feature2]].corr().iloc[0, 1]\n",
    "            \n",
    "            results.append({\n",
    "                'Feature 1': feature1,\n",
    "                'Feature 2': feature2,\n",
    "                'Pearson Correlation': np.abs(corr)\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    ordered_results_df = results_df.sort_values(by='Pearson Correlation', ascending=False)\n",
    "    ordered_results_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return ordered_results_df\n",
    "\n",
    "pearson = calculate_pearson_correlation(numerical, numerical, train_data_transformed)\n",
    "pearson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical - Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "def perform_anova_test(numerical, categorical, alpha=0.001):\n",
    "    results = []\n",
    "\n",
    "    for num_feature in numerical:\n",
    "        for cat_feature in categorical:\n",
    "            groups = [train_data_transformed[train_data_transformed[cat_feature] == level][num_feature] for level in train_data_transformed[cat_feature].unique()]\n",
    "            f_stat, p_value = f_oneway(*groups)\n",
    "            \n",
    "            results.append({\n",
    "                'Numerical Feature': num_feature,\n",
    "                'Categorical Feature': cat_feature,\n",
    "                'F-statistic': f_stat,\n",
    "                'P-value': p_value\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    filtered_results_df = results_df[results_df['P-value'] < alpha]\n",
    "    ordered_results_df = filtered_results_df.sort_values(by='F-statistic', ascending=False)\n",
    "    ordered_results_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return ordered_results_df\n",
    "\n",
    "anova = perform_anova_test(numerical, categorical)\n",
    "anova"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical - Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "def calculate_chi2(categorical_features, data, alpha=0.001):\n",
    "    results = []\n",
    "\n",
    "    for cat_feature1 in categorical_features:\n",
    "        for cat_feature2 in categorical_features:\n",
    "            if cat_feature1 == cat_feature2:\n",
    "                continue\n",
    "            \n",
    "            contingency_table = pd.crosstab(data[cat_feature1], data[cat_feature2])\n",
    "            chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "            \n",
    "            results.append({\n",
    "                'Categorical Feature 1': cat_feature1,\n",
    "                'Categorical Feature 2': cat_feature2,\n",
    "                'Chi-Square Statistic': chi2_stat,\n",
    "                'P-value': p_value\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    filtered_results_df = results_df[results_df['P-value'] < alpha]\n",
    "    ordered_results_df = filtered_results_df.sort_values(by='Chi-Square Statistic', ascending=False)\n",
    "    ordered_results_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return ordered_results_df\n",
    "\n",
    "chi2 = calculate_chi2(categorical, train_data_transformed)\n",
    "chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Features - Target Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson = calculate_pearson_correlation(numerical, TARGET_VARIABLES, train_data_transformed)\n",
    "pearson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Features - Target Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_results_df = perform_anova_test(TARGET_VARIABLES, categorical)\n",
    "ordered_results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
